diff --git a/common/log.h b/common/log.h
index d13f72d8..cc17474a 100644
--- a/common/log.h
+++ b/common/log.h
@@ -79,10 +79,17 @@ void gpt_log_set_timestamps(struct gpt_log * log,       bool   timestamps); // w
 #define LOG(...)             LOG_TMPL(GGML_LOG_LEVEL_NONE, 0,         __VA_ARGS__)
 #define LOGV(verbosity, ...) LOG_TMPL(GGML_LOG_LEVEL_NONE, verbosity, __VA_ARGS__)
 
+#if 1
+#define LOG_INF(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR,  0,                 __VA_ARGS__)
+#define LOG_WRN(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR,  0,                 __VA_ARGS__)
+#define LOG_ERR(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR, 0,                 __VA_ARGS__)
+#define LOG_DBG(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR,  0,                 __VA_ARGS__)
+#else
 #define LOG_INF(...) LOG_TMPL(GGML_LOG_LEVEL_INFO,  0,                 __VA_ARGS__)
 #define LOG_WRN(...) LOG_TMPL(GGML_LOG_LEVEL_WARN,  0,                 __VA_ARGS__)
 #define LOG_ERR(...) LOG_TMPL(GGML_LOG_LEVEL_ERROR, 0,                 __VA_ARGS__)
 #define LOG_DBG(...) LOG_TMPL(GGML_LOG_LEVEL_DEBUG, LOG_DEFAULT_DEBUG, __VA_ARGS__)
+#endif
 
 #define LOG_INFV(verbosity, ...) LOG_TMPL(GGML_LOG_LEVEL_INFO,  verbosity, __VA_ARGS__)
 #define LOG_WRNV(verbosity, ...) LOG_TMPL(GGML_LOG_LEVEL_WARN,  verbosity, __VA_ARGS__)
diff --git a/examples/main/main.cpp b/examples/main/main.cpp
index c3041f1f..92cebbd9 100644
--- a/examples/main/main.cpp
+++ b/examples/main/main.cpp
@@ -544,7 +544,9 @@ int main(int argc, char ** argv) {
 
     while ((n_remain != 0 && !is_antiprompt) || params.interactive) {
         // predict
-        if (!embd.empty()) {
+        LOG_DBG("========= ccdev: embd.empty()=%d, n_remain=%d, params.n_predict=%d, ga_n=%d, params.n_batch=%d\n",
+                embd.empty(), n_remain, params.n_predict, ga_n, params.n_batch);
+        if (!embd.empty()) { // ccdev: NOT enter here
             // Note: (n_ctx - 4) here is to match the logic for commandline prompt handling via
             // --prompt or --file which uses the same value.
             int max_embd_size = n_ctx - 4;
@@ -559,7 +561,7 @@ int main(int argc, char ** argv) {
                 console::set_display(console::reset);
             }
 
-            if (ga_n == 1) {
+            if (ga_n == 1) { // ccdev: enter here
                 // infinite text generation via context shifting
                 // if we run out of context:
                 // - take the n_keep first tokens from the original prompt (via n_past)
@@ -620,6 +622,7 @@ int main(int argc, char ** argv) {
 
             // try to reuse a matching prefix from the loaded session instead of re-eval (via n_past)
             if (n_session_consumed < (int) session_tokens.size()) {
+                LOG_DBG("ccdev ========= fuck here ========\n");
                 size_t i = 0;
                 for ( ; i < embd.size(); i++) {
                     if (embd[i] != session_tokens[n_session_consumed]) {
@@ -640,6 +643,7 @@ int main(int argc, char ** argv) {
                 }
             }
 
+            LOG_DBG("ccdev: embd.size()=%zu\n", embd.size());
             for (int i = 0; i < (int) embd.size(); i += params.n_batch) {
                 int n_eval = (int) embd.size() - i;
                 if (n_eval > params.n_batch) {
@@ -670,6 +674,7 @@ int main(int argc, char ** argv) {
 
         embd.clear();
 
+        llama_token sampleToken;
         if ((int) embd_inp.size() <= n_consumed && !is_interacting) {
             // optionally save the session on first sample (for faster prompt loading next time)
             if (!path_session.empty() && need_to_save_session && !params.prompt_cache_ro) {
@@ -679,10 +684,23 @@ int main(int argc, char ** argv) {
                 LOG_DBG("saved session to %s\n", path_session.c_str());
             }
 
+#if 0
+            const auto * logits = llama_get_logits_ith(ctx, -1);
+            const int n_vocab = llama_n_vocab(llama_get_model(ctx));
+            llama_token id = 0;
+            float maxV = logits[0];
+            for (int i = 0; i < n_vocab; i++) {
+                if (logits[i] > maxV) {
+                    maxV = logits[i];
+                    id = i;
+                }
+            }
+            sampleToken = id;
+#else
             const llama_token id = gpt_sampler_sample(smpl, ctx, -1);
 
             gpt_sampler_accept(smpl, id, /* accept_grammar= */ true);
-
+#endif
             // LOG_DBG("last: %s\n", string_from(ctx, smpl->prev.to_vector()).c_str());
 
             embd.push_back(id);
@@ -727,7 +745,7 @@ int main(int argc, char ** argv) {
                 } else {
                     // Outgoing Generated Tokens
                     output_tokens.push_back(id);
-                    output_ss << token_str;
+                    output_ss << token_str; // TODO: what the fuck
                 }
             }
         }
@@ -866,7 +884,9 @@ int main(int argc, char ** argv) {
                     const auto line_inp = ::llama_tokenize(ctx, user_inp,            false, format_chat);
                     const auto line_sfx = ::llama_tokenize(ctx, params.input_suffix, false, true);
 
+                    LOG_DBG("line_pfx tokens: %s\n", string_from(ctx, line_pfx).c_str());
                     LOG_DBG("input tokens: %s\n", string_from(ctx, line_inp).c_str());
+                    LOG_DBG("line_sfx tokens: %s\n", string_from(ctx, line_sfx).c_str());
 
                     // if user stop generation mid-way, we must add EOT to finish model's last response
                     if (need_insert_eot && format_chat) {
@@ -879,6 +899,8 @@ int main(int argc, char ** argv) {
                     embd_inp.insert(embd_inp.end(), line_inp.begin(), line_inp.end());
                     embd_inp.insert(embd_inp.end(), line_sfx.begin(), line_sfx.end());
 
+                    LOG_DBG("ALL embd_inp tokens: %s\n", string_from(ctx, embd_inp).c_str());
+
                     for (size_t i = original_size; i < embd_inp.size(); ++i) {
                         const llama_token token = embd_inp[i];
                         output_tokens.push_back(token);
@@ -888,6 +910,7 @@ int main(int argc, char ** argv) {
                     // reset assistant message
                     assistant_ss.str("");
 
+                    LOG_DBG("n_remain: %d, line_inp.size()=%zu\n", n_remain, line_inp.size());
                     n_remain -= line_inp.size();
                     LOG_DBG("n_remain: %d\n", n_remain);
                 } else {
diff --git a/ggml/src/ggml.c b/ggml/src/ggml.c
index 201d5466..20292ab5 100644
--- a/ggml/src/ggml.c
+++ b/ggml/src/ggml.c
@@ -20538,13 +20538,15 @@ struct ggml_tensor * ggml_graph_get_tensor(struct ggml_cgraph * cgraph, const ch
         }
     }
 
+    printf("ccdev: --------begin----- %s ------------\n", __func__);
     for (int i = 0; i < cgraph->n_nodes; i++) {
         struct ggml_tensor * node = cgraph->nodes[i];
-
+        printf("ccdev: node[%d] = \t%s\n", i, node->name);
         if (strcmp(node->name, name) == 0) {
             return node;
         }
     }
+    printf("ccdev: --------end----- %s ------------\n", __func__);
 
     return NULL;
 }
diff --git a/src/llama.cpp b/src/llama.cpp
index af8afd84..3a04f424 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -16798,6 +16798,10 @@ static int llama_decode_internal(
         struct ggml_tensor * res  = ggml_graph_node(gf, -1);
         struct ggml_tensor * embd = ggml_graph_node(gf, -2);
 
+#if 1 // qwen2
+        struct ggml_tensor * aft_norm  = ggml_graph_get_tensor(gf, "attn_norm-1");
+#endif
+
         if (lctx.n_outputs == 0) {
             // no output
             res  = nullptr;
@@ -16839,6 +16843,20 @@ static int llama_decode_internal(
         //    ggml_graph_dump_dot(gf, NULL, "llama.dot");
         //}
 
+#if 1 // qwen2
+        if (aft_norm) {
+            for (int i = 0; i <GGML_MAX_DIMS; i++) {
+                printf("ccdev: ----------dim[%d]= %ld ------------\n", i, aft_norm->ne[i]);                
+            }
+            float* tmp = reinterpret_cast<float*>(aft_norm->data);
+            for (int i = 0; i < 5; i++) {
+                printf("ccdev: ------------- %f ------------\n", tmp[i]);
+            }
+        } else {
+            printf("ccdev: ------------aft_norm == NULL ------\n");
+        }
+#endif
+
         // extract logits
         if (res) {
             ggml_backend_t backend_res = ggml_backend_sched_get_tensor_backend(lctx.sched, res);
@@ -18610,7 +18628,7 @@ void llama_backend_init(void) {
     {
         struct ggml_init_params params = { 0, NULL, false };
         struct ggml_context * ctx = ggml_init(params);
-        ggml_free(ctx);
+        ggml_free(ctx); // ccdev: why free
     }
 }
 
@@ -20669,12 +20687,12 @@ void llama_synchronize(struct llama_context * ctx) {
     // this should only happen when using batch size 1 to evaluate a batch
 
     // add the evaluation to the stats
-    if (ctx->n_queued_tokens == 1) {
+    if (ctx->n_queued_tokens == 1) { // decode
         if (!ctx->cparams.no_perf) {
             ctx->t_eval_us += ggml_time_us() - ctx->t_compute_start_us;
         }
         ctx->n_eval++;
-    } else if (ctx->n_queued_tokens > 1) {
+    } else if (ctx->n_queued_tokens > 1) { // prompt
         if (!ctx->cparams.no_perf) {
             ctx->t_p_eval_us += ggml_time_us() - ctx->t_compute_start_us;
         }
